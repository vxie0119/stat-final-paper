---
title: "Decoding the Impact: Statistical Analysis of Factors Contributing to NFL Injuries"
author: "Vincent Xie"
toc: false
number-sections: false
execute:
    echo: false
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    fontsize: 12pt
    geometry: 
      - top=30mm
      - left=20mm
header-includes:
  - \usepackage{setspace}
  - \doublespacing
##  docx: default
---
```{python}
# Packages
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy.stats import chi2_contingency
import numpy as np
```

# Abstract

This paper investigates the diverse factors that significantly influence the occurrence and characteristics of injuries within the National Football League (NFL). Using a dataset that encompasses injury records, player activities, and environmental conditions, this research explores the interplay between player positions, play types, game conditions (including weather and surface type), and their relationship to injury occurrences. Employing statistical techniques such as logistic regression, Chi-square tests, and ANOVA, the study seeks to identify significant correlations and causative factors that can inform injury prevention strategies. The goal is to enhance understanding of injury dynamics in professional football and contribute to improved player safety measures. Insights gained may challenge existing assumptions and lead to innovative health strategies in sports management, significantly impacting player care and game integrity.

# Introduction

In the realm of professional sports, particularly within the National Football League (NFL), injuries are an inevitable consequence of high-impact and physically demanding activities. The incidence and severity of these injuries not only affect player health and career longevity but also influence team dynamics and league operations. This study focuses on a comprehensive analysis of various factors that might contribute to the occurrence and nature of injuries, specifically lower body extremities, in the NFL, aiming to identify patterns and potential preventative measures. By identify the preventative measures, athletes can worry less about how injuries during their career will affect their life after. The importance of this research lies not only in enhancing player safety but also in advancing the broader understanding of injury prevention in contact sports.

Prior research in sports medicine and athletic injuries has primarily focused on immediate causes and rehabilitation of injuries. Within the NFL, the annual health and safety reports provide aggregate data on injury types and frequencies, which serve as a baseline for this study. However, there is a lack of comprehensive analysis that integrates environmental conditions, play strategies, and player-specific variables to predict and analyze injury occurrences comprehensively. There is also research on how the surface type affects the frequency in lower extremity injuries. The results show a positive correlation between the two factors. This paper will also perform statistical analysis on how specific factors affect the frequency of lower extremity injuries. It may support the claims of prior research or provide new insight on other causes that may contribute to these injuries.

While existing studies have laid a solid foundation in understanding the factors influencing sports injuries, gaps remain in the holistic analysis of how combined factors like weather conditions, playing surface type, player position, and specific play scenarios contribute to the risk and nature of injuries. Most studies have not fully utilized the granular data now available through player tracking technologies and detailed game logs, which can offer deeper insights into the conditions under which injuries most frequently occur.

This paper contributes to the existing literature by employing statistical techniques to analyze a unique dataset combining player injury records, play-by-play data, and environmental conditions. By doing so, it aims to:

+ Quantify the impact of various factors on the likelihood and severity of injuries.
+ Identify specific conditions or scenarios that significantly increase injury risk.
+ Offer recommendations for injury prevention based on empirical evidence.

The remainder of this paper is structured as follows:

1. Data Description and Cleaning: This section details the datasets used, sources, variables of interest, and the nature of the data. If there are irrelevant variables or emptiness, it will be removed or filled in.
2. Research Design and Methods: Here, the statistical methods applied in the analysis, such as logistic regression and ANOVA, are discussed along with the rationale behind their use.
3. Results: This section presents the findings of the study, discussing how various factors correlate with injury occurrences and their implications.
4. Discussion: Insights and interpretations of the results are discussed, along with their implications for injury prevention strategies in the NFL.
5. Conclusion: The paper concludes with a summary of the findings and their contributions to sports injury research and practice, along with suggestions for future research.

# Data

The data utilized in this study comes from a [Kaggle](https://www.kaggle.com/competitions/nfl-playing-surface-analytics/data) data set sponsored by the National Football League (NFL). The files include injurys records and a list of information that detail the conditions of the play and information about the location and weather. There is another data set that displays detailed player movement metrics but the file is simply too large. For the purpose of this paper, we will utilize the first two datasets. The dataset covers two NFL seasons from recent years, although specific years are not detailed. This period is crucial for understanding current injury trends and preventive measures.

This data is integral to analyzing the occurrence and nature of injuries in the NFL because it includes variables directly related to the research questions, such as player positions, field types, weather conditions, and specific injury details. These variables allow for a nuanced analysis of how different conditions correlate with injury risks. 

Preliminary exploratory analysis would involve:

+ Descriptive statistics to summarize the data, such as average injury durations, frequency of injuries by body part, and common player positions associated with injuries.
+ Visualization techniques, including heatmaps of injuries by field type and line graphs of injury occurrences over time.
+ Correlation matrices to identify potential relationships between variables such as weather conditions and injury rates.

This structured approach will guide readers through the foundational data used in the study, establishing the groundwork for more detailed statistical analysis and interpretation in subsequent sections of your paper.

Data cleaning is a necessary step to examine whether or not there is incomplete data. 

```{python}
# Load the datasets
injury = pd.read_csv('data/InjuryRecord.csv')
plays = pd.read_csv('data/PlayList.csv')
new_injury = pd.read_csv('data/mod_InjuryRecord.csv')
```

```{python}
# Summarize missing data for InjuryRecord.csv
injury_missing_summary = injury.isnull().sum()
injury_missing_summary = injury_missing_summary[injury_missing_summary > 0].rename('InjuryRecord')

# Summarize missing data for PlayList.csv
playlist_missing_summary = plays.isnull().sum()
playlist_missing_summary = playlist_missing_summary[playlist_missing_summary > 0].rename('PlayList')

# Calculate the ratio of missing data to total entries for each dataset
missing_data_ratio_injury = (injury_missing_summary.sum() / len(injury)) * 100
missing_data_ratio_playlist = (playlist_missing_summary.sum() / len(plays)) * 100

# Create a DataFrame from the missing data summaries
missing_data_summary = pd.DataFrame({
    'InjuryRecord.csv': injury_missing_summary,
    'PlayList.csv': playlist_missing_summary
})

# Fill NaN values with zero and convert all numeric counts to integers
missing_data_summary = missing_data_summary.fillna(0).astype(int)

# Convert all numeric counts to integers (ensures whole numbers)
missing_data_summary = missing_data_summary.astype(int)

# Add the ratio of missing data as a new row
missing_data_summary.loc['Missing Data Ratio (%)'] = [f"{missing_data_ratio_injury:.2f}%", f"{missing_data_ratio_playlist:.2f}%"]

# Transpose the DataFrame for better readability
missing_data_summary = missing_data_summary.T

# Rename index to indicate it's the file name and adjust column names if needed
missing_data_summary.index.name = 'File Name'

# Display the DataFrame neatly
print(missing_data_summary)
```

From `InjuryRecord.csv`, there are only 28 values missing in the `PlayKey` column from the original 105. At 27%, there is a significant amount of missing data entries. However, it would be difficult to fill in the missing entries without using an external dataset. This variable is also within the `PlayList.csv` file so it can be used to fill in the data in `InjuryRecord.csv` (if applicable). In `PlayList.csv`, 13% of the data is missing, mainly in `StadiumType`, `Weather`, and `PlayType`. Similar to before, it is difficult to fill in data without an external resource. For now, the models and analysis will use the data available in the files.

```{python}
# Fill missing 'PlayKey' values with 'GameID' concatenated with '-1'
injury['PlayKey'] = injury.apply(
    lambda row: f"{row['GameID']}-1" if pd.isnull(row['PlayKey']) else row['PlayKey'],
    axis=1
)

# Save the modified dataframe if needed
injury.to_csv('data/mod_InjuryRecord.csv', index=False)
```

For the missing entries in `InjuryRecord.csv`, we can fill in the missing data by formatting the missing values based on the `GameID`. This step allows us to locate the data between each file and determine important factors that contribute to injuries.

```{python}
# Bar chart for frequency of stadium types in PlayList
stadium_freq = plays['StadiumType'].value_counts()
plt.figure(figsize=(5, 2))
stadium_freq.plot(kind='bar', color='lightgreen')
plt.title('Frequency of Stadium Types')
plt.xlabel('Stadium Type')
plt.ylabel('Frequency')
plt.show()
```

This graph models all of the different stadium types that are in the file. Notice how there are many repeats that can be combined. In order to reduce repetitiveness, these values can be cleaned by combining like-values.

```{python}
# A quick view at the 'StadiumType' unique values to understand the variations
stadium_types = plays['StadiumType'].unique()

stadium_type_mapping = {
    'Retr. Roof-Closed': 'Retr. Roof - Closed',
    'Retr. Roof-Open': 'Retr. Roof - Open',
    'Outdoor': 'Outdoors',
    'Ourdoor': 'Outdoors',
    'Oudoor': 'Outdoors',
    'Outddors': 'Outdoors',
    'Outdor': 'Outdoors',
    'Outside': 'Outdoors',
    'Heinz Field': 'Outdoors',  # Assuming Heinz Field is always outdoor
    'Cloudy': 'Outdoors',  # Assuming 'Cloudy' is an outdoor condition and not a stadium type
    'Indoor': 'Indoors',
    'Domed': 'Dome',
    'Domed, closed': 'Dome, closed',
    'Domed, Open': 'Dome, closed',
    'Closed Dome': 'Dome, closed',
    'Retr. Roof Closed': 'Retr. Roof - Closed'
    # More mappings can be added here based on the actual data
}

# Standardize the 'StadiumType' column
plays['StadiumType'] = plays['StadiumType'].replace(stadium_type_mapping)

# Save the modified dataframe if needed
plays.to_csv('data/mod_PlayList.csv', index=False)

# Calculate the new frequency of each stadium type
cleaned_stadium_freq = plays['StadiumType'].value_counts()

# Bar chart for frequency of stadium types in PlayList
stadium_freq = plays['StadiumType'].value_counts()
plt.figure(figsize=(6, 2))
stadium_freq.plot(kind='bar', color='lightgreen')
plt.title('Frequency of Stadium Types')
plt.xlabel('Stadium Type')
plt.ylabel('Frequency')
plt.show()
```

We can also
```{python}
# Define the temperature bins and labels
bins = [-float('inf'), 50, 60, 70, 80, float('inf')]
labels = ['Cold', 'Cool', 'Mild', 'Warm', 'Hot']

# Replace the temperature values with categorical labels directly
plays['Temperature'] = pd.cut(plays['Temperature'], bins=bins, labels=labels)

# Save the modified dataframe if needed
plays.to_csv('data/mod_PlayList.csv', index=False)
```

```{python}
# Mapping function for weather descriptions
def map_weather(weather):
    if pd.isna(weather):
        return 'Unknown'
    weather = weather.lower()
    if any(word in weather for word in ['rain', 'showers', 'rainy']):
        return 'Rain'
    elif any(word in weather for word in ['overcast', 'cloudy']):
        return 'Overcast'
    elif any(word in weather for word in ['clear', 'sunny']):
        return 'Clear'
    elif 'snow' in weather:
        return 'Snow'
    elif any(word in weather for word in ['indoor', 'indoors', 'controlled climate', 'n/a']):
        return 'None'
    return 'Unknown'

# Apply the mapping function to the Weather column
plays['WeatherCategory'] = plays['Weather'].apply(map_weather)

# Save the modified dataframe if needed
plays.to_csv('data/mod_PlayList.csv', index=False)

# Verify the changes by displaying the value counts
plays['WeatherCategory'].value_counts()
```

# Modeling

```{python}
# Bar chart for frequency of each injury type in InjuryRecord
injury_freq = new_injury['BodyPart'].value_counts()
plt.figure(figsize=(5, 2))
injury_freq.plot(kind='bar', color='skyblue')
plt.title('Frequency of Each Injury')
plt.xlabel('Body Part')
plt.ylabel('Frequency')
plt.show()

# Merge injury_record and playlist on PlayerKey
merged_data = pd.merge(new_injury, plays, on='PlayKey', how='inner')

# Calculate the frequency of injuries by roster position
injury_position_frequency = merged_data['RosterPosition'].value_counts()

# Plotting the frequency of injuries by roster position
plt.figure(figsize=(8, 4))  # Smaller plot size for better readability
injury_position_frequency.plot(kind='bar', color='salmon')
plt.title('Frequency of Injuries by Roster Position')
plt.xlabel('Roster Position')
plt.ylabel('Frequency')
plt.tight_layout()  # Adjust layout to fit elements
plt.show()
```

```{python}
# Merge injury_record and playlist on PlayerKey
merged_data = pd.merge(new_injury, plays, on='PlayKey', how='inner')

# Group by 'RosterPosition' and 'BodyPart' and count the occurrences
injury_by_position = merged_data.groupby(['RosterPosition', 'BodyPart']).size().reset_index(name='Frequency')

# Create a pivot table with 'RosterPosition' as index, 'BodyPart' as columns, and 'Frequency' as values
pivot_table = injury_by_position.pivot_table(index='RosterPosition', columns='BodyPart', values='Frequency', fill_value=0)

# Convert frequencies to integers
pivot_table = pivot_table.astype(int)

# Plot a heatmap to visualize the frequency of each type of injury by each position group
plt.figure(figsize=(14, 8))
sns.heatmap(pivot_table, annot=True, cmap="viridis")
plt.title('Frequency of Each Type of Injury by Position Group')
plt.xlabel('Body Part')
plt.ylabel('Roster Position')
plt.show()
```

# Methods

```{python}
# Create a set of unique keys from InjuryRecord which indicates injuries
injury_keys = set(new_injury['PlayKey'])

# Create a binary variable in PlayList that indicates whether an injury occurred
plays['InjuryOccurred'] = plays['PlayKey'].isin(injury_keys).astype(int)

# Ensure 'Surface' column is in the PlayList dataset and check data
if 'FieldType' not in plays.columns:
    print("Error: 'FieldType' column is missing from PlayList dataset.")
else:
    # Create a contingency table of injuries by surface type
    contingency_table = pd.crosstab(plays['InjuryOccurred'], plays['FieldType'])

    # Perform the Chi-Square test
    chi2, p, dof, expected = chi2_contingency(contingency_table)

    print(f"Chi-Square Statistic: {chi2:.3f}, p-value: {p:.3f}")
```

```{}
# If 'FieldType' is not already coded as 0/1, convert it to numeric codes
plays['FieldType'] = plays['FieldType'].astype('category').cat.codes

# Filter the DataFrame to include only cases where an injury occurred
injury_cases = plays[plays['InjuryOccurred'] == 1]

# The independent variable should be 'FieldType', and we add an intercept to it
X = injury_cases[['FieldType']]  # Assuming 'FieldType' is already coded as 0/1
X = sm.add_constant(X)  # This adds the intercept

# The dependent variable is 'InjuryOccurred' (though in this subset, it will always be 1)
y = injury_cases['InjuryOccurred']

# Perform logistic regression
logit_model = sm.Logit(y, X)
result = logit_model.fit(maxiter=100)  # Increase the maximum number of iterations

# Display summary results
print(result.summary())
```

```{}
# Convert categorical variables to dummy variables, including 'FieldType', 'RosterPosition', and 'Weather'
# Be sure to handle missing data first, if any
plays.dropna(inplace=True)  # This removes any rows with missing data
plays = pd.get_dummies(plays, columns=['FieldType', 'RosterPosition', 'Weather'], drop_first=True)

# Ensure all columns are of a numeric data type
for col in plays.columns:
    if plays[col].dtype == 'object':
        plays[col] = plays[col].astype('category').cat.codes  # This converts object types to categorical

# Define the dependent and independent variables
y = plays['InjuryOccurred']
X = plays.drop('InjuryOccurred', axis=1)

# Add a constant to the independent variable set for the intercept
X = sm.add_constant(X)

# Convert X and y to numpy arrays, ensuring they are numeric
X = np.asarray(X, dtype=float)
y = np.asarray(y, dtype=float)

# Perform logistic regression
logit_model = sm.Logit(y, X)
result = logit_model.fit()

# Display summary results
print(result.summary())
```

# Simulation

# Application

# Conclusion

# Websites for reference

+ https://pubmed.ncbi.nlm.nih.gov/30452873/